Kine - Technical Requirements Document (TRD)
Target Builder: Claude Code (Cursor) / AI Agent
Project Type: Real-Time Accessibility PWA / Mobile Web App (Android/iOS Compatible)
Tech Stack: Next.js 14+ (App Router), TypeScript, Tailwind CSS, Supabase, Google MediaPipe (Web), Google Gemini 3.0 API (Multimodal), AWS GenASL, ElevenLabs API.
Date: January 14, 2026
Last Updated: January 15, 2026 (Gemini Sandwich Architecture + AWS GenASL)
1. System Architecture Overview
Core Philosophy
The application acts as a bi-directional bridge. It is "State-Driven" rather than "Page-Driven," relying on two primary modes (SIGNING_MODE vs. LISTENING_MODE) within a single dynamic view to reduce latency.
Tech Stack Definitions
Frontend: Next.js (React) optimized for Mobile Viewport (PWA).
Styling: Tailwind CSS (Mobile-first, Accessibility-focused classes).
State Management: Zustand (for global mode switching and preference storage).
Vision AI (Edge): Google MediaPipe task-vision package (running in a Web Worker to prevent UI freezing).
Generative AI - "The Gemini Sandwich":
  - Input (SIGNING_MODE): Google Gemini 3.0 Multimodal - interprets MediaPipe landmarks as ASL â†’ English text.
  - Output (LISTENING_MODE): Google Gemini 3.0 - translates English text â†’ ASL Gloss for avatar playback.
Avatar Engine: AWS GenASL - generates video avatar clips from ASL gloss sequences using Amazon Bedrock, Step Functions, and CloudFront.
Audio AI (Server): ElevenLabs API for text-to-speech (SIGNING_MODE output).
Database: Supabase (PostgreSQL).
Storage: Supabase Storage (flipbook frames - legacy fallback).
2. UI/UX Specifications
Instructions for AI: Strictly follow these layouts and UX laws.
UX Principles (The Framework)
Don't Make Me Think (Steve Krug): The UI has only two states. No complex navigation menus.
Fitts's Law: The "Mode Switch" button must be massive and located in the bottom "Thumb Zone".
Jakob's Law:
Signing Mode = Mental Model of a "Camera App".
Listening Mode = Mental Model of "Siri/Voice Assistant".
Accessibility: All text must be text-yellow-400 on bg-black (High Contrast) by default.
View 1: SIGNING_MODE (The Input Interface)
Layout Structure:
Background Layer (Z-0): A full-screen video element (<CameraFeed />) covering 100vh/100vw.
Overlay Layer (Z-10): A <canvas> element (<HandTracker />) perfectly aligned with the video feed to draw the red skeleton overlays.
UI Layer (Z-20):
Top Bar: A transparent flex container positioned absolute top. Contains a "History" icon (left) and "Settings" icon (right).
Translation Box: A floating text container (<TranscriptionBox />) positioned in the lower-middle of the screen (above the controls). It displays the real-time inference text.
Status Indicator: A pulsing ring animation that overlays the center screen when isProcessing is true.
Bottom Control Bar: A fixed bottom container taking up the bottom 20% of the viewport. Background is semi-transparent black gradient.
Primary Action: Inside the bottom bar, a massive (h-20 w-20 or larger) circular button (<ModeToggle />) with a Microphone Icon.
Interaction: Tapping the button triggers setMode('LISTENING').
View 2: LISTENING_MODE (The Receiver Interface)
Layout Structure:
Background Layer (Z-0): Solid Black (bg-black).
Content Container (Z-10): A Flex column centered vertically.
Transcription Area: Large, High-Contrast Text (text-4xl text-yellow-400 font-bold) left-aligned at the top of the container.
Avatar Display: The <AvatarPlayer /> component centered in the middle. It renders ASL avatar videos via AWS GenASL (primary) or flipbook frames (fallback). When idle, it shows a "breathing" animation. When active, it plays video clips generated by GenASL for each gloss in the sequence.
Waveform: The <Waveform /> component positioned below the avatar. It animates CSS bars based on microphone input volume to provide visual feedback that the app is "hearing."
Bottom Control Bar (Z-20):
Identical positioning to View 1.
Primary Action: The button transforms to show a Hand/Palm Icon.
Interaction: Tapping the button triggers setMode('SIGNING') and re-initializes the camera.
3. File & Folder Structure
Instructions for the AI: Establish this structure immediately to ensure modularity.

/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ layout.tsx       # Global wrappers (Toaster, Theme)
â”‚   â”œâ”€â”€ page.tsx         # Main entry (Mode Switcher Logic)
â”‚   â”œâ”€â”€ globals.css      # Tailwind directives + Animations
â”‚   â””â”€â”€ auth/            # Login/Signup routes (Phase 5)
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ camera/
â”‚   â”‚   â”œâ”€â”€ HandTracker.tsx    # MediaPipe Canvas overlay
â”‚   â”‚   â””â”€â”€ CameraFeed.tsx     # Raw video element
â”‚   â”œâ”€â”€ avatar/
â”‚   â”‚   â”œâ”€â”€ AvatarPlayer.tsx   # Main avatar wrapper (mode: genasl | flipbook | legacy)
â”‚   â”‚   â”œâ”€â”€ GenASLPlayer.tsx   # AWS GenASL video-based avatar player
â”‚   â”‚   â””â”€â”€ FlipbookPlayer.tsx # Canvas-based 24fps frame animation (fallback)
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ ModeToggle.tsx     # The "Thumb Zone" button
â”‚   â”‚   â”œâ”€â”€ Waveform.tsx       # CSS Animation for listening
â”‚   â”‚   â””â”€â”€ TranscriptionBox.tsx # High contrast text display
â”‚   â””â”€â”€ modals/
â”‚       â”œâ”€â”€ SettingsModal.tsx
â”‚       â””â”€â”€ HistoryModal.tsx
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ mock-data/       # JSON mocks for development
â”‚   â”‚   â”œâ”€â”€ avatars.json # Mock gloss mappings
â”‚   â”‚   â””â”€â”€ history.json # Mock conversation logs
â”‚   â”œâ”€â”€ supabase/        # Client instantiation
â”‚   â”œâ”€â”€ mediapipe/       # Worker setup for landmark detection
â”‚   â”œâ”€â”€ gemini/          # Gemini 3.0 API - "The Gemini Sandwich"
â”‚   â”‚   â”œâ”€â”€ prompts.ts       # ASL gloss translation prompts
â”‚   â”‚   â”œâ”€â”€ translationService.ts # English â†’ ASL Gloss
â”‚   â”‚   â”œâ”€â”€ signRecognitionService.ts # Landmarks â†’ English (Multimodal)
â”‚   â”‚   â””â”€â”€ index.ts         # Barrel exports
â”‚   â”œâ”€â”€ aws/             # AWS GenASL integration
â”‚   â”‚   â”œâ”€â”€ config.ts        # AWS environment configuration
â”‚   â”‚   â”œâ”€â”€ genASLService.ts # GenASL API client (Step Functions, S3, CloudFront)
â”‚   â”‚   â””â”€â”€ index.ts         # Barrel exports
â”‚   â”œâ”€â”€ elevenlabs/      # Audio conversion services
â”‚   â”œâ”€â”€ avatar/          # Flipbook avatar system (legacy fallback)
â”‚   â”‚   â”œâ”€â”€ types.ts         # FlipbookEntry, FlipbookState types
â”‚   â”‚   â”œâ”€â”€ flipbookService.ts # Frame loading, caching, Supabase Storage
â”‚   â”‚   â””â”€â”€ index.ts         # Barrel exports
â”‚   â””â”€â”€ utils.ts         # Formatting helpers
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useCamera.ts     # Stream management
â”‚   â”œâ”€â”€ useAudio.ts      # Mic recording & permissions
â”‚   â”œâ”€â”€ useFlipbook.ts   # Flipbook playback timing (requestAnimationFrame)
â”‚   â””â”€â”€ useTranslation.ts# Orchestrator for API calls
â”œâ”€â”€ scripts/             # Python data pipeline
â”‚   â”œâ”€â”€ extract_frames.py     # FFmpeg frame extraction from How2Sign
â”‚   â”œâ”€â”€ upload_to_supabase.py # Upload frames to Supabase Storage
â”‚   â”œâ”€â”€ generate_sample_frames.py # Generate placeholder frames
â”‚   â””â”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ store/
â”‚   â”œâ”€â”€ useAppStore.ts   # Global state (CurrentMode, IsProcessing)
â”‚   â””â”€â”€ useUserStore.ts  # Preferences (TextSize, AvatarMode)
â””â”€â”€ __tests__/           # Co-located tests per module

4. Database Schema & Types (Supabase)
Note: During Phases 1-4, define these as TypeScript Interfaces only. Implement actual Tables in Phase 5.
Table: users
id: UUID (PK) - references auth.users
role: 'deaf' | 'hearing' | 'blind'
preferences: JSONB { "visual_mode": "text_plus_avatar", "voice_id": "string", "high_contrast": boolean }
Table: avatar_library (Flipbook System)
gloss_label: String (PK/Unique Index) - e.g., "COFFEE"
frame_count: Integer - Number of WebP frames for this gloss
fps: Integer (default 24) - Playback frame rate
storage_path: String - Path in Supabase Storage (e.g., "avatars/COFFEE")
video_url: String (legacy, nullable) - For backwards compatibility
category: String
metadata: JSONB (e.g., duration_ms, signer_id, dialect, source)
Table: messages
id: UUID (PK)
session_id: UUID
direction: 'sign_to_audio' | 'audio_to_sign'
original_text: String (The raw input)
translated_text: String (The final output)
gloss_sequence: String[] (Array of Gloss keys for debug/playback)
audio_url: String (Nullable)
5. Module Specifications & Logic Flow
Module A: Camera & MediaPipe (The Input Eye)
Responsibility: Capture video, extract vector data, perform NO rendering logic (pure data).
Initialization: useCamera hook requests navigator.mediaDevices.getUserMedia.
Processing:
Instantiate FilesetResolver and HandLandmarker.
CRITICAL: Also instantiate FaceLandmarker for grammatical context (eyebrows/mouth).
Optimization: Run inference every 100ms (not every frame) to save battery.
Output: Stream a JSON object: { hands: Landmark[], face: Landmark[] } to the useTranslation hook.
Test Requirement: Mock getUserMedia and ensure the hook returns error if permission denied.
Module B: The Gemini Sandwich (The Brain) ðŸ¥ª
Responsibility: Gemini 3.0 powers BOTH sides of the bidirectional bridge.

**SIGNING_MODE Flow (Gemini as "The Eyes"):**
```
Camera â†’ MediaPipe Landmarks â†’ Gemini 3.0 Multimodal â†’ English Text â†’ ElevenLabs TTS â†’ Audio
```
- Buffering: Accumulate landmarks for 1.5-2 seconds (sliding window).
- Trigger: When hand motion < threshold (user stops signing), fire API request.
- Payload: Send landmark history + video frames to Gemini 3.0 Multimodal.
- Prompt: "Interpret these ASL hand/face landmarks and return the English translation."
- Output: English text for display + ElevenLabs audio synthesis.

**LISTENING_MODE Flow (Gemini as "The Linguist"):**
```
Microphone â†’ Speech-to-Text â†’ Gemini 3.0 â†’ ASL Gloss Array â†’ GenASL â†’ Avatar Video
```
- Input: Transcribed speech text.
- Prompt: "Translate this English text to ASL gloss notation."
- Output: Array of gloss keys (e.g., ["HELLO", "HOW", "YOU"]).
- Handoff: Pass gloss array to GenASL for video avatar generation.

Test Requirement: Write a unit test that pushes mock landmark data and asserts the correct API payload is formed for both flows.
Module C: AWS GenASL Avatar Engine (The Output Eye)
Responsibility: Rendering ASL avatar videos from gloss sequences using AWS GenASL.
Input: Receives an array of Gloss keys from Gemini: ['WHERE', 'COFFEE'].

**Primary Architecture (AWS GenASL):**
```
Gemini Gloss Output â†’ GenASL API â†’ Step Functions â†’ Video Generation â†’ CloudFront â†’ Video Player
```
Logic:
1. Receive gloss array from Gemini translation.
2. Call GenASL API endpoint with text/gloss sequence.
3. GenASL orchestrates: Bedrock (optional re-translation) â†’ Lambda â†’ S3 â†’ CloudFront.
4. Poll execution status via Step Functions ARN.
5. Retrieve pre-signed video URL from CloudFront.
6. Play video in GenASLPlayer component.
7. On video complete, play next item in queue.

**Fallback Architecture (Flipbook - Legacy):**
```
Gloss Array â†’ Supabase Storage â†’ WebP Frames â†’ Canvas (24fps)
```
- Used when AWS GenASL is not configured.
- Query avatar_library for frame_count, fps, storage_path.
- Preload frames and play at 24fps using requestAnimationFrame.

**Avatar Mode Priority:**
1. `genasl` - AWS GenASL video (if configured)
2. `flipbook` - Supabase Storage frames (fallback)
3. `legacy` - Emoji placeholder animations

Test Requirement: Mock GenASL API responses and ensure video playback triggers correctly.
Module D: UI/UX & State (The Interaction)
Responsibility: "Don't Make Me Think" implementation.
Global Store (useAppStore):
mode: 'SIGNING' | 'LISTENING'
isProcessing: boolean
lastTranslation: string
Mode Toggle:
Clicking the Bottom Bar Button triggers setMode.
Transition: Use Framer Motion AnimatePresence to cross-fade the entire view.
Accessibility:
All text must use Tailwind classes text-yellow-400 bg-black (High Contrast) if user.preferences.high_contrast is true.
6. Development Phases for Agent
Phase 1: Skeleton & UI Layout (Local Only)
Goal: A working interactive wireframe with no real backend.
Tasks:
Setup Next.js + Tailwind.
Build ModeToggle component.
Build the "Listening" view (Black screen + Yellow text) using hardcoded strings.
Build the "Signing" view (Camera feed placeholder).
Verify: User can click the bottom button to swap views smoothly.
Phase 2: MediaPipe Integration (Local Only)
Goal: Get the camera working and tracking hands.
Tasks:
Implement useCamera to get real video stream.
Connect MediaPipe HandLandmarker.
Draw red skeleton overlays on a canvas on top of the video.
Verify: Moving hand in front of camera updates the red lines in real-time.
Phase 3: Avatar Engine âœ… COMPLETE (Flipbook) / ðŸ”„ IN PROGRESS (GenASL)
Goal: Implement avatar playback system with AWS GenASL as primary and flipbook as fallback.
Tasks:
âœ… Set up Supabase Storage bucket "avatars" for WebP frames (fallback).
âœ… Create Python scripts for frame extraction and upload.
âœ… Build FlipbookPlayer component with canvas-based rendering.
âœ… Implement flipbookService for frame URL generation and caching.
ðŸ”„ Integrate AWS GenASL for video-based avatar playback.
ðŸ”„ Build GenASLPlayer component for video rendering.
ðŸ”„ Implement genASLService for API communication.
Verify: Calling playGenASL('Hello world') translates and plays GenASL video.
Phase 4: The Gemini Sandwich Integration
Goal: Implement bidirectional Gemini 3.0 integration.
Tasks:
Implement Gemini Multimodal for SIGNING_MODE (landmarks â†’ English).
Implement Gemini translation for LISTENING_MODE (English â†’ ASL gloss).
Connect GenASL to receive gloss from Gemini output.
Full loop: Sign â†’ Gemini interprets â†’ Audio output.
Full loop: Speak â†’ Gemini translates â†’ GenASL avatar plays.
Verify: Both directions of the bridge work end-to-end.
Phase 5: Backend & API Injection (Final Polish)
Goal: Connect all services and polish the experience.
Tasks:
Initialize Supabase Client & Auth.
Configure AWS GenASL environment variables.
Integrate ElevenLabs for text-to-speech output.
Add .env.local variables for all services.
Verify: Full bidirectional translation works with real APIs.
7. Variables & Configuration Constants
config/constants.ts
USE_MOCK_DATA: false (Now using real APIs)
LANDMARK_SAMPLING_RATE: 100 (ms)
SILENCE_TRIGGER_THRESHOLD: 1500 (ms of no motion to trigger translation)
MAX_BUFFER_SIZE: 50 (frames)

**Flipbook Settings (Fallback):**
FRAME_DURATION_MS: 41.67 (1000ms / 24fps)
DEFAULT_FPS: 24 (flipbook playback rate)
SUPABASE_STORAGE_BUCKET: "avatars"

**AWS GenASL Settings:**
GENASL_API_ENDPOINT: (from env) AWS API Gateway URL
GENASL_POLL_INTERVAL: 2000 (ms between status checks)
GENASL_MAX_POLL_ATTEMPTS: 30 (max polling iterations)
GENASL_AVATAR_STYLES: ['realistic', 'cartoon', 'minimal']
GENASL_VIDEO_QUALITIES: ['low', 'medium', 'high'] (480p, 720p, 1080p)
GENASL_SPEED_OPTIONS: [0.5, 0.75, 1.0, 1.25, 1.5]

**Environment Variables (.env.local):**
GEMINI_API_KEY: Google Gemini 3.0 API key
NEXT_PUBLIC_AWS_GENASL_API_ENDPOINT: AWS GenASL API Gateway URL
NEXT_PUBLIC_AWS_REGION: AWS region (e.g., us-east-1)
NEXT_PUBLIC_AWS_COGNITO_IDENTITY_POOL_ID: Cognito Identity Pool
NEXT_PUBLIC_AWS_CLOUDFRONT_URL: CloudFront distribution URL
ELEVENLABS_API_KEY: ElevenLabs TTS API key
8. Testing Strategy (Mandatory)
Unit Tests (vitest):
Test utils.ts formatters.
Test useAppStore state changes.
Component Tests (react-testing-library):
AvatarPlayer: Ensure video src changes when prop updates.
ModeToggle: Ensure click fires the state change function.
Integration Tests:
Phases 1-4: Verify mock data flows correctly.
Phase 5: Verify that fetchTranslation calls the correct Edge Function URL.

Mermaid Chart - The Gemini Sandwich Architecture
graph TD
    subgraph Client_Device [User's Mobile Device]
        UI[User Interface]
        Cam[Camera Input]
        Mic[Microphone Input]
        MP[MediaPipe Hand + Face Mesh]
        AudioPlayer[Audio Player]
        GenASLPlayer[GenASL Video Player]
        FlipbookEngine[Flipbook Engine - Fallback]
    end

    subgraph Backend_Services [Backend & API Layer]
        API[API Gateway / Edge Functions]
        Auth[Supabase Auth]
        DB[(Supabase DB)]
        Storage[(Supabase Storage - Fallback)]
    end

    subgraph Gemini_Sandwich [ðŸ¥ª The Gemini Sandwich]
        GeminiEyes[Gemini 3.0 Multimodal<br/>ASL â†’ English]
        GeminiLinguist[Gemini 3.0<br/>English â†’ ASL Gloss]
    end

    subgraph AWS_GenASL [AWS GenASL Stack]
        GenASLAPI[API Gateway]
        StepFunc[Step Functions]
        S3[(S3 Video Storage)]
        CloudFront[CloudFront CDN]
    end

    subgraph Audio_Services [Audio Services]
        Eleven[ElevenLabs TTS]
    end

    %% SIGNING_MODE Flow (Gemini as The Eyes)
    Cam -->|Video Stream| MP
    MP -->|Landmarks + Frames| GeminiEyes
    GeminiEyes -->|English Text| UI
    GeminiEyes -->|English Text| Eleven
    Eleven -->|Audio Stream| AudioPlayer

    %% LISTENING_MODE Flow (Gemini as The Linguist)
    Mic -->|Audio Input| UI
    UI -->|Speech-to-Text| API
    API -->|English Text| GeminiLinguist
    GeminiLinguist -->|ASL Gloss Array| GenASLAPI
    GenASLAPI -->|Execute| StepFunc
    StepFunc -->|Generate Video| S3
    S3 -->|Serve| CloudFront
    CloudFront -->|Video URL| GenASLPlayer

    %% Fallback Flow (Flipbook)
    GeminiLinguist -.->|Fallback: Gloss Array| Storage
    Storage -.->|WebP Frames| FlipbookEngine

    %% Data Persistence
    API -.->|Log History| DB
    UI -.->|Authenticate| Auth

    %% Styling
    style GeminiEyes fill:#4285F4,color:#fff
    style GeminiLinguist fill:#4285F4,color:#fff
    style GenASLAPI fill:#FF9900,color:#000
    style StepFunc fill:#FF9900,color:#000
    style CloudFront fill:#FF9900,color:#000
